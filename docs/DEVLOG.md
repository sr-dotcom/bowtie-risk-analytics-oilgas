# Dev Log

## 2026-02-01 - Initial Setup
Set up the project structure with `src/`, `tests/`, and `data/` directories.
Added Pydantic models for incident data and basic tests.
Configured gitignore to exclude local env files.

## 2026-02-02 - Proposal Updates
Updated the proposal based on feedback:
- Narrowed MVP scope to "Loss of Containment" scenarios only.
- Will focus on Logistic Regression vs XGBoost for the model comparison.
- Added SHAP/reason codes for explainability.
- Target dataset size: ~200 labeled examples.

## Next Steps
- Implement JSON schema validation for Bowtie data.
- Build the initial data ingestion pipeline.

## 2026-02-04 - Phase 1 & 2 Implementation
Implemented core data foundation and analytics engine:
- **Schema**: Defined Pydantic models for `Incident`, `Threat`, `Barrier`, `Consequence`, and `Bowtie`.
- **Ingestion**: Created a text loader to parse raw incident narratives and extract barrier information.
- **Analytics**: Implemented logic to calculate barrier coverage (prevention/mitigation) and identify gaps against a reference Bowtie.
- **Pipeline**: Built an end-to-end processing script (`src/pipeline.py`) that orchestrates ingestion and analytics.
- **Verification**: Validated the pipeline with sample data and a "Loss of Containment" Bowtie definition.

## 2026-02-04 - Streamlit MVP & App Hardening
Implemented Streamlit MVP and stabilized end-to-end demo flow:
- Added app data-loading utilities and tests to reliably read pipeline outputs.
- Built Streamlit dashboard KPIs and an Incident Explorer for per-incident barrier coverage and gap details.
- Hardened the UI against missing optional fields in incident JSON (safe defaults, no KeyErrors).
- Updated .gitignore to prevent committing local planning artifacts.

Verification:
- Unit tests pass (`pytest`)
- Pipeline runs successfully (`python -m src.pipeline`)
- Streamlit renders dashboard and incident explorer (`streamlit run src/app/main.py`)

## 2026-02-05 â€” Step 1.2.2: Initial Data Acquisition (CSB/BSEE)

Implemented a manifest-driven acquisition workflow for public incident reports:
- Added incident/text manifest models with CSV load/save utilities.
- Implemented CSB and BSEE discovery + PDF download with streaming and SHA256 hashing.
- Added PDF-to-text extraction using pdfplumber and a text manifest for extraction results.
- Extended the pipeline CLI with `acquire` and `extract-text` subcommands while preserving the original `process` behavior.
- Added unit tests for manifests, sources, PDF extraction, and CLI parsing.

Validation:
- `pytest -q` passes.
- CLI smoke tests: `python -m src.pipeline --help`, `python -m src.pipeline acquire --help`, acquisition + download + extract-text run end-to-end.

